{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression on MNIST in Numpy Tutorial\n",
    "\n",
    "In this notebook, we will write a script to perform a linear regression on the MNIST dataset from scratch in Numpy, without a framework like Tensorflow or Keras. For the sake of consistency with more complex networks, we will use Stochastic Gradient Descent (SGD) to perform the regression, instead of the more efficient least squares optimization. This will allow us to add one more layer and make a logistic regression classifier in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "This will be a duplicate of my other tutorial on CNNs, so the description will be the same. We will load the data, perform a per-pixel normalization, and encode the labels. To start, we'll import the MNIST data without one-hot encoding - we'll do that ourselves later. If you'd rather avoid one-hot encoding manually, just set `one_hot=True` and make the necessary adjustments. We will also load the data manually, even though just using the mnist.train.next_batch command can be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data # downloads MNIST images\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False) \n",
    "\n",
    "data, labels = mnist.train.next_batch(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset into train and validation datasets, and preprocess them. We subtract the per-pixel mean (the average of each pixel over the entire training set, which sets the mean to zero, and the standard deviation, in this case, to about .25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.000055, Std: 0.259276\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(50000, 28, 28, 1)\n",
    "\n",
    "temp_data = data - np.mean(data, axis=0)\n",
    "\n",
    "train_dataset = temp_data[:45000].reshape(45000, 784)\n",
    "train_labels = labels[:45000]\n",
    "\n",
    "valid_dataset = temp_data[45000:].reshape(5000, 784)\n",
    "valid_labels = labels[45000:]\n",
    "\n",
    "print(\"Mean: %f, Std: %f\" % (np.mean(train_dataset), np.std(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify data\n",
    "\n",
    "We can use the matplotlib.pyplot module to view some of the sample images and their corresponding labels. The data is encoded by defaut as a single `(784, 1)` Numpy array, so we need to manually reshape it in order to display the correct image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125a4cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n = random.randint(0, len(valid_dataset))\n",
    "\n",
    "image = valid_dataset[n] #mnist.train.next_batch(1)\n",
    "print(\"Number: {}\".format(valid_labels[n]))\n",
    "\n",
    "plt.imshow(image.reshape(28, 28))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities:\n",
    "\n",
    "These are utilities for processing the data and handling the process of training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels:\n",
    "\n",
    "We can use a very concise but somewhat sophisticated script to one-hot encode the labels. The inner statement in parenthesis uses Numpy broadcasting to convert the arange array, with shape `(10, )`, into an array of shape `(10, len(labels))`, and the labels array, with shape `(len(labels),)`, into an array with shape `(len(labels), 1)`. The `[:,None]` syntax creates a new axis with length 1. The two arrays are then compared elementwise, returning a boolean array which is `True` in the desired position. We then cast the boolean values as integers, to get our one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    return (np.arange(10) == labels[:,None]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5 Encoded label:  [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_labels_encoded = encode_labels(train_labels)\n",
    "valid_labels_encoded = encode_labels(valid_labels)\n",
    "\n",
    "print(\"Label: \", train_labels[2], \"Encoded label: \", train_labels_encoded[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches from preprocessed data:\n",
    "\n",
    "This is pretty self explanatory. Given a dataset and the corresponding labels, it generates a random set of examples of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(dataset, labels, batch_size):\n",
    "    indices = random.sample(range(len(dataset)), batch_size)\n",
    "    batch_data = dataset[indices]\n",
    "    batch_labels = labels[indices].reshape(batch_size,10)\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Accuracy\n",
    "\n",
    "This will compute the argmax (index of maximum element of predicted encoded labels), and compare that to the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(size, weights, biases):\n",
    "    data, labels = next_batch(valid_dataset, valid_labels_encoded, size)\n",
    "    prediction = predict(data, weights, biases)\n",
    "    accuracy = (np.argmax(prediction, axis=1) == np.argmax(labels, axis=1)).sum() * 100 / size\n",
    "    print(\"The accuracy of your model is %s%%!\" % accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "This is the meat of the linear regression optimization, and it is presented in two implementations. The first is an iterative version, which is more intuitive but far less efficient than the vectorized version. Numpy has a number of high efficiency vectorized operations which are precompiled in C, and we can take advantage of the compiled code if we write it without loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is just linear regression, our model is very simple. We just have a single matrix, with shape `10x784`, which we multiply by the pixels in our image to predict a probability distribution over our 10 classes (and a bias). The gradient of a linear transformation is extremely simple: it's just the matrix itself. So for each weight, we just have to compute the gradient of the loss. We will use an L2 loss, so the calculus looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L = \\frac{1}{2}\\sum_{i}^{\\text{batch size}}\\sum_j^{10} (p_{ij} - \\text{label}_{ij})^2$$\n",
    "\n",
    "where $p_{ij}$ is the $j$th component of the $i$th prediction in the minibatch. Specifically,\n",
    "\n",
    "$$p_i = W\\cdot x_i + b$$\n",
    "\n",
    "where $x_i$ is the $i$th image in the dataset. We add the one-half to cancel out the square during differentiation. Therefore:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{m n}} = \\sum_{i}^{\\text{batch size}} (p_{im} - \\text{label}_{im})\\cdot x_{in}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the learning rate, which controls how much the weights are updated at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "iterations = 401\n",
    "\n",
    "#learning_rate = .2*np.exp(-3*np.arange(0,iterations) / iterations) # exponential learning rate\n",
    "# learning_rate = np.array([.05*(1 - x/iterations) for x in range(iterations)]) # linear learning rate\n",
    "learning_rate = .2* np.ones(iterations) # linear learning rate\n",
    "\n",
    "\n",
    "def predict(data, weights, biases):\n",
    "    #print(\"Predict data:\", np.argmax(data[0:5], axis=1))\n",
    "    \"\"\"Predicts an encoded label for a given image (multiplies the image by the weight matrix and adds the bias).\"\"\"\n",
    "    return np.dot(data, weights.T) + biases.T\n",
    "\n",
    "def linear_regression(batch_size, iterations, learning_rate):\n",
    "    \"\"\"For i iterations, computes the loss, and then calculates the gradient for each of the weights and biases.\"\"\"\n",
    "    \n",
    "    weights = np.random.randn(10, 784) # initialize weights to normal distribution\n",
    "    biases = np.random.randn(10)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        data, labels = next_batch(train_dataset, train_labels_encoded, batch_size)\n",
    "        prediction = predict(data, weights, biases) \n",
    "        error_arr = labels - prediction\n",
    "        loss = np.tensordot(error_arr, error_arr) / (2*batch_size)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Loss at step %s is %s\" % (i, loss))\n",
    "        if i % 50 == 0:\n",
    "            evaluate(512, weights, biases)\n",
    "        dW = np.zeros_like(weights)\n",
    "        dB = np.zeros_like(biases)\n",
    "        for j in range(np.shape(weights)[0]):\n",
    "            for k in range(np.shape(weights)[1]):\n",
    "                dW[j,k]= - np.sum((labels-prediction)[:, j] * data[:, k]) / batch_size\n",
    "        for j in range(len(biases)):\n",
    "            dB[j]= - np.sum((labels-prediction)[:, j]) / batch_size\n",
    "\n",
    "        weights = weights - learning_rate[i]*dW\n",
    "        biases = biases - learning_rate[i]*dB\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 300.194849182\n",
      "The accuracy of your model is 9.765625%!\n",
      "Loss at step 10 is 54.1172580969\n",
      "Loss at step 20 is 35.3476123198\n",
      "Loss at step 30 is 27.8820963894\n",
      "Loss at step 40 is 22.2641059254\n",
      "Loss at step 50 is 19.2610970945\n",
      "The accuracy of your model is 12.890625%!\n",
      "Loss at step 60 is 17.4446407037\n",
      "Loss at step 70 is 14.9998902915\n",
      "Loss at step 80 is 13.8840358746\n",
      "Loss at step 90 is 12.6696528459\n",
      "Loss at step 100 is 11.9472870476\n",
      "The accuracy of your model is 18.9453125%!\n",
      "Loss at step 110 is 11.1995934117\n",
      "Loss at step 120 is 9.76998217169\n",
      "Loss at step 130 is 9.34830128248\n",
      "Loss at step 140 is 9.25743411949\n",
      "Loss at step 150 is 9.14742682457\n",
      "The accuracy of your model is 18.359375%!\n",
      "Loss at step 160 is 8.05281726146\n",
      "Loss at step 170 is 7.83602981463\n",
      "Loss at step 180 is 7.53155884156\n",
      "Loss at step 190 is 6.95065720402\n",
      "Loss at step 200 is 7.02660445715\n",
      "The accuracy of your model is 25.0%!\n",
      "Loss at step 210 is 6.48889249862\n",
      "Loss at step 220 is 6.04254583501\n",
      "Loss at step 230 is 6.24252149485\n",
      "Loss at step 240 is 5.80604810139\n",
      "Loss at step 250 is 5.79681754398\n",
      "The accuracy of your model is 25.0%!\n",
      "Loss at step 260 is 5.5121807485\n",
      "Loss at step 270 is 5.16721680664\n",
      "Loss at step 280 is 5.00539918302\n",
      "Loss at step 290 is 4.81926479437\n",
      "Loss at step 300 is 4.61905445275\n",
      "The accuracy of your model is 23.6328125%!\n",
      "Loss at step 310 is 4.54092319499\n",
      "Loss at step 320 is 4.4565757348\n",
      "Loss at step 330 is 4.39269638006\n",
      "Loss at step 340 is 3.9507685057\n",
      "Loss at step 350 is 3.87019181571\n",
      "The accuracy of your model is 27.9296875%!\n",
      "Loss at step 360 is 3.60652736538\n",
      "Loss at step 370 is 3.63928615966\n",
      "Loss at step 380 is 3.58833675337\n",
      "Loss at step 390 is 3.78674311888\n"
     ]
    }
   ],
   "source": [
    "weights, biases = linear_regression(batch_size, iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, linear regression really isn't doing a great job of capturing the complexity of the MNIST dataset. However, it can be a very useful tool, and we'll find that with the addition of a logistic layer, it can perform quite well. But before that, let's vectorize this example.\n",
    "\n",
    "**Vectorization** is the process of converting an iterative/loop based algorithm into something that uses Numpy's built-in, pre-compiled methods. The code is much faster, and much more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "iterations = 2001\n",
    "\n",
    "learning_rate = .4*np.exp(-5*np.arange(0,iterations) / iterations) # exponential learning rate\n",
    "# learning_rate = np.array([.05*(1 - x/iterations) for x in range(iterations)]) # linear learning rate\n",
    "#learning_rate = .2* np.ones(iterations) # linear learning rate\n",
    "\n",
    "def vectorized_linear_regression(batch_size, iterations, learning_rate):\n",
    "    \n",
    "    weights = np.random.randn(10, 784) # initialize weights to normal distribution\n",
    "    biases = np.random.randn(10, 1)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        data, labels = next_batch(train_dataset, train_labels_encoded, batch_size)\n",
    "        prediction = predict(data, weights, biases)\n",
    "        error_arr = labels - prediction\n",
    "        loss = np.tensordot(error_arr, error_arr) / (2*batch_size)\n",
    "        if i % 50 == 0: print(\"Loss at step %s is %s\" % (i, loss))\n",
    "        dW = - np.sum(error_arr[..., None] * data[:, None, :], axis=0) / batch_size\n",
    "        dB = - np.sum(error_arr, axis=0)[:, None] / batch_size\n",
    "        weights = weights - learning_rate[i]*dW\n",
    "        biases = biases - learning_rate[i]*dB\n",
    "        \n",
    "        if i % 300 == 0:\n",
    "            evaluate(512, weights, biases)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 269.230131634\n",
      "The accuracy of your model is 9.765625%!\n",
      "Loss at step 50 is 12.7985945259\n",
      "Loss at step 100 is 7.58873564243\n",
      "Loss at step 150 is 5.62362972117\n",
      "Loss at step 200 is 4.54804419135\n",
      "Loss at step 250 is 3.61121634433\n",
      "Loss at step 300 is 3.13486886243\n",
      "The accuracy of your model is 27.1484375%!\n",
      "Loss at step 350 is 2.83787208575\n",
      "Loss at step 400 is 2.77736766216\n",
      "Loss at step 450 is 2.49060247774\n",
      "Loss at step 500 is 2.21586026204\n",
      "Loss at step 550 is 2.17008208553\n",
      "Loss at step 600 is 2.11486219364\n",
      "The accuracy of your model is 36.5234375%!\n",
      "Loss at step 650 is 1.99350500395\n",
      "Loss at step 700 is 2.03029032889\n",
      "Loss at step 750 is 1.91548918287\n",
      "Loss at step 800 is 1.91941634693\n",
      "Loss at step 850 is 1.76436628185\n",
      "Loss at step 900 is 1.74610586957\n",
      "The accuracy of your model is 41.015625%!\n",
      "Loss at step 950 is 1.70749525562\n",
      "Loss at step 1000 is 1.84194906092\n",
      "Loss at step 1050 is 1.7252152309\n",
      "Loss at step 1100 is 1.66007152463\n",
      "Loss at step 1150 is 1.8182176767\n",
      "Loss at step 1200 is 1.66613629432\n",
      "The accuracy of your model is 44.140625%!\n",
      "Loss at step 1250 is 1.68994557097\n",
      "Loss at step 1300 is 1.70741214932\n",
      "Loss at step 1350 is 1.72599866424\n",
      "Loss at step 1400 is 1.58031046666\n",
      "Loss at step 1450 is 1.50418628067\n",
      "Loss at step 1500 is 1.50043822848\n",
      "The accuracy of your model is 42.96875%!\n",
      "Loss at step 1550 is 1.72135414721\n",
      "Loss at step 1600 is 1.58738831108\n",
      "Loss at step 1650 is 1.7394122544\n",
      "Loss at step 1700 is 1.70393479706\n",
      "Loss at step 1750 is 1.61325564081\n",
      "Loss at step 1800 is 1.69353509071\n",
      "The accuracy of your model is 45.1171875%!\n",
      "Loss at step 1850 is 1.58750564142\n",
      "Loss at step 1900 is 1.54129767461\n",
      "Loss at step 1950 is 1.57178105436\n"
     ]
    }
   ],
   "source": [
    "weights, biases = vectorized_linear_regression(batch_size, iterations, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much faster this method is, and how much more quickly it converges to a useful model. This is the power of vectorization, and why Numpy is such a great tool for any sort of scientific computing in Python. See my next notebook on Logistic regression, where we'll take this a step further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
