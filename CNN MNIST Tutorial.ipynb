{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Convolution Classifier Tutorial with Hidden Layer Visualization\n",
    "\n",
    "This notebook is intended as an introductory tutorial to convolution networks and the MNIST dataset. This network can be used to achieve good results on the Kaggle MNIST challenge, and the hidden layer visualization reveals parts of the network's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing Data:\n",
    "\n",
    "An increasingly prevalent step in image classification networks is preprocessing and batch normalization. For this dataset, we will limit our preprocessing to rescaling and normalizing the mean and standard deviation, but please check out my preprocessing tutorial for a more elaborate approach. \n",
    "\n",
    "To start, we'll import the MNIST data without one-hot encoding - we'll do that ourselves later. If you'd rather avoid one-hot encoding manually, just set `one_hot=True` and make the necessary adjustments. We will also load the data manually, even though just using the mnist.train.next_batch command can be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data # downloads MNIST to Desktop\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False) \n",
    "\n",
    "data, labels = mnist.train.next_batch(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then divide the dataset into train and validation datasets, and preprocess them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.000046, Std: 0.259397\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(50000, 28, 28, 1)\n",
    "\n",
    "temp_data = data - np.mean(data, axis=0)\n",
    "\n",
    "train_dataset = temp_data[:45000]\n",
    "train_labels = labels[:45000]\n",
    "\n",
    "valid_dataset = temp_data[45000:]\n",
    "valid_labels = labels[45000:]\n",
    "\n",
    "print(\"Mean: %f, Std: %f\" % (np.mean(train_dataset), np.std(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify data\n",
    "\n",
    "We can use the matplotlib.pyplot module to view some of the sample images and their corresponding labels. The data is encoded by defaut as a single `(784, 1)` Numpy array, so we need to manually reshape it in order to display the correct image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bdeed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n = random.randint(0, len(valid_dataset))\n",
    "\n",
    "image = valid_dataset[n] #mnist.train.next_batch(1)\n",
    "print(\"Number: {}\".format(valid_labels[n]))\n",
    "\n",
    "plt.imshow(image.reshape(28, 28))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Class Distribution:\n",
    "\n",
    "An important step when dealing with any sort of classification is to ensure that our data is relatively uniform across classes. We can visualize the class distribution using a histogram provided my the matplotlib module. You can see that the labels are actually not uniformly distributed, and we could prune the labels if we so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkKSOGQKS1DFDQJI6ZghIUscMAUnq2P8BSFD52P6YFWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b0d0c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities:\n",
    "\n",
    "These are utilities for processing the data and handling the process of training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode labels:\n",
    "\n",
    "We can use a very concise but somewhat sophisticated script to one-hot encode the labels. The inner statement in parenthesis uses Numpy broadcasting to convert the arange array, with shape `(10, )`, into an array of shape `(10, len(labels))`, and the labels array, with shape `(len(labels),)`, into an array with shape `(len(labels), 1)`. The `[:,None]` syntax creates a new axis with length 1. The two arrays are then compared elementwise, returning a boolean array which is `True` in the desired position. We then cast the boolean values as integers, to get our one-hot encoded labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    return (np.arange(10) == labels[:,None]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5 Encoded label:  [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_labels_encoded = encode_labels(train_labels)\n",
    "valid_labels_encoded = encode_labels(valid_labels)\n",
    "\n",
    "print(\"Label: \", train_labels[2], \"Encoded label: \", train_labels_encoded[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches from preprocessed data:\n",
    "\n",
    "This is pretty self explanatory. Given a dataset and the corresponding labels, it generates a random set of examples of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(dataset, labels, batch_size):\n",
    "    indices = random.sample(range(len(dataset)), batch_size)\n",
    "    batch_data = dataset[indices]\n",
    "    batch_labels = labels[indices].reshape(batch_size,10)\n",
    "    return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate validation accuracy:\n",
    "\n",
    "The importance of a validation set in training neural networks cannot be overstated. Think about a trendline: high correlation with a linear trendline is statistically significant, but if you make the fit more and more elaborate, it's less and less meaningful. **It's easy to overfit data**, and a validation set which evaluates the accuracy impartially is essential. You can cut off training when validation accuracy stops improving, and use that information to make better models.\n",
    "\n",
    "This function evaluates the percent accuracy for a batch of a given size from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(size):\n",
    "    images_valid, labels_valid = next_batch(valid_dataset, valid_labels_encoded, size)\n",
    "    percent = model.evaluate(images_valid.reshape(size, vsize, hsize, num_channels), labels_valid)[1]*100\n",
    "    print(\"Your model is {} percent accurate!\".format(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup:\n",
    "\n",
    "We begin the training process by importing the relevant layers and activations from Keras. In Keras, the Sequential API is designed to be easy to use with linear neural networks, which most simple convolution networks are. However, more complicated networks, like the Google Inception architecture, are not at all sequential, and the functional API is more appropriate there. There are four models below, and you are welcome to run each of them, or only one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Conv2DTranspose, Reshape, Flatten, Conv2D, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "image_size = [28, 28, 1]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "vsize = image_size[0]\n",
    "hsize = image_size[1]\n",
    "num_channels = image_size[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Fully Connected Model\n",
    "\n",
    "The simplest MNIST models use a series of fully connected layers to make predictions. These fully connected layers are just large matrix multiplications applied to the entire image, followed by non-linear activations. They are easy to use and their derivatives are simple, making them a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(vsize, hsize, num_channels)))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu')) # we can also specify the activation in the previous layer, i.e. Dense(512, activation='relu')\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify the optimizer which Keras will use to train the model. We will use the classic Stochastic Gradient Descent optimizer, including a momentum parameter and using Nesterov predictive momentum. More information on these optimizers can be found in [this](http://ruder.io/optimizing-gradient-descent/index.html#adam) excellent guide. We compile the model, and use the summary method show the number of parameters in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 550,346\n",
      "Trainable params: 550,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.0001, clipvalue=1.0, decay=3e-8)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary() # this will list the layers and number of parameters for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Here we actually train the model. For additional control, we perform this step manually, batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 96.875 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.0216376\n",
      "model loss is  0.113933\n",
      "model loss is  0.00380781\n",
      "model loss is  0.0137464\n",
      "model loss is  0.00420753\n",
      "Iter:  1000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 96.875 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.108884\n",
      "model loss is  0.02225\n",
      "model loss is  0.00289289\n",
      "model loss is  0.0172207\n",
      "model loss is  0.0300012\n",
      "Iter:  2000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 100.0 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.12044\n",
      "model loss is  0.0326128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-c239ce5854f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    956\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m    957\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 896\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    897\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1279\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1280\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1283\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1263\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "for i in range(num_steps):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iter: \", i)\n",
    "        evaluate(batch_size)\n",
    "        print('\\n')\n",
    "    \n",
    "    images_train, labels_train = next_batch(train_dataset, train_labels_encoded, batch_size)\n",
    "\n",
    "    loss = model.train_on_batch(images_train.reshape(batch_size, vsize, hsize, num_channels), labels_train) \n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"model loss is \", loss[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked pretty well. But let's try something different. **Before continuing, please continue down to the \"Evaluate Model\" header if you want to save predictions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small convolution model\n",
    "\n",
    "This is a small convolution model, which can be trained in a matter of minutes. This model uses a series of convolution layers, followed by ReLU activations, and Dropout to prevent overfitting. In the final step, GlobalAveragePooling is used, which takes the average of each layer, turning a spatially-large vector into a `1x1xn` prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution networks are a more natural way of dealing with image classification than fully connected networks, because they preserve spatial information about the input image. GlobalAveragePooling also has the advantage of preserving information separated out by the feature map, and encouraging the network to produce a feature map that describes the image. Convolution layers are also much smaller than fully connected layers, since they're only connected to a small region of the image at a time, and share weights across the whole image. Look how much more complex this network looks, even though it uses about a fifth as many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 1)         10        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 28)        56        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 28, 28, 28)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 28)        7084      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 28, 28, 28)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 28)        7084      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 28)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 14, 28)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 56)        14168     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 56)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 56)        28280     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 56)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 56)          28280     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 56)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 56)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 56)          28280     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 7, 7, 56)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 56)          3192      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 56)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 10)          570       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 117,004\n",
      "Trainable params: 117,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(1, 3, strides=1, padding = 'same', input_shape=(vsize, hsize, num_channels))) # for visualization\n",
    "model.add(LeakyReLU(0.2))\n",
    "\n",
    "model.add(Conv2D(28, 1, strides=1, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Conv2D(28, 3, strides=1, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Conv2D(28, 3, strides=2, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(56, 3, strides=1, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Conv2D(56, 3, strides=1, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Conv2D(56, 3, strides=2, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(56, 3, strides=1, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Conv2D(56, 1, strides=1, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "model.add(Conv2D(10, 1, strides=2, padding = 'same'))\n",
    "model.add(LeakyReLU(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.0002) # here we use the Adam optimizer, a more elaborate cousin of SGD\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Here we actually train the model. For additional control, we perform this step manually, batch by batch. To choose which model to train, just change model to `lconv_model`, `sconv_model`, or `fc_model`. Note that while this model uses fewer parameters than the fully connected model, it takes much longer to train, since backpropogation is much more complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 3.125 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  2.30261\n",
      "model loss is  1.4226\n",
      "model loss is  1.0183\n",
      "model loss is  0.709499\n",
      "model loss is  0.661462\n",
      "Iter:  1000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 75.0 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.436884\n",
      "model loss is  0.612463\n",
      "model loss is  0.513328\n",
      "model loss is  0.433636\n",
      "model loss is  0.280018\n",
      "Iter:  2000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 96.875 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.326926\n",
      "model loss is  0.525506\n",
      "model loss is  0.577065\n",
      "model loss is  0.518575\n",
      "model loss is  0.530569\n",
      "Iter:  3000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 90.625 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.163107\n",
      "model loss is  0.142122\n",
      "model loss is  0.151462\n",
      "model loss is  0.173525\n",
      "model loss is  0.0709266\n",
      "Iter:  4000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 96.875 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.359121\n",
      "model loss is  0.339858\n",
      "model loss is  0.0282522\n",
      "model loss is  0.185371\n",
      "model loss is  0.321326\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "for i in range(num_steps):\n",
    "    if i % 500 == 0:\n",
    "        print(\"Iter: \", i)\n",
    "        evaluate(batch_size)\n",
    "        print('\\n')\n",
    "    \n",
    "    images_train, labels_train = next_batch(train_dataset, train_labels_encoded, batch_size)\n",
    "\n",
    "    loss = model.train_on_batch(images_train.reshape(batch_size, vsize, hsize, num_channels), labels_train) \n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"model loss is \", loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large convolution model\n",
    "\n",
    "This is a much larger convolution model, and it may take a while to train on a CPU. The structure is mostly the same, except the hidden layers are deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_96 (Conv2D)           (None, 28, 28, 1)         10        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 28, 28, 84)        168       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)   (None, 28, 28, 84)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 28, 28, 84)        63588     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)   (None, 28, 28, 84)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 14, 14, 84)        63588     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)   (None, 14, 14, 84)        0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 14, 14, 84)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 14, 14, 168)       127176    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_97 (LeakyReLU)   (None, 14, 14, 168)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 14, 14, 168)       254184    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_98 (LeakyReLU)   (None, 14, 14, 168)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 7, 7, 168)         254184    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_99 (LeakyReLU)   (None, 7, 7, 168)         0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 7, 7, 168)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 7, 7, 168)         254184    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_100 (LeakyReLU)  (None, 7, 7, 168)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 7, 7, 168)         28392     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_101 (LeakyReLU)  (None, 7, 7, 168)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_105 (Conv2D)          (None, 4, 4, 10)          1690      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_102 (LeakyReLU)  (None, 4, 4, 10)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,047,164\n",
      "Trainable params: 1,047,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lconv_model = Sequential()\n",
    "\n",
    "lconv_model.add(Conv2D(1, 3, strides=1, padding = 'same', input_shape=(vsize, hsize, num_channels))) # for visualization\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "\n",
    "lconv_model.add(Conv2D(84, 1, strides=1, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "lconv_model.add(Conv2D(84, 3, strides=1, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "lconv_model.add(Conv2D(84, 3, strides=2, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "\n",
    "lconv_model.add(Dropout(0.3))\n",
    "\n",
    "lconv_model.add(Conv2D(168, 3, strides=1, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "lconv_model.add(Conv2D(168, 3, strides=1, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "lconv_model.add(Conv2D(168, 3, strides=2, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "\n",
    "lconv_model.add(Dropout(0.4))\n",
    "\n",
    "lconv_model.add(Conv2D(168, 3, strides=1, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "lconv_model.add(Conv2D(168, 1, strides=1, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "lconv_model.add(Conv2D(10, 1, strides=2, padding = 'same'))\n",
    "lconv_model.add(LeakyReLU(0.2))\n",
    "\n",
    "lconv_model.add(GlobalAveragePooling2D())\n",
    "\n",
    "lconv_model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.0002) # here we use the Adam optimizer, a more elaborate cousin of SGD\n",
    "\n",
    "lconv_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "lconv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Here we actually train the model. For additional control, we perform this step manually, batch by batch. To choose which model to train, just change model to `lconv_model`, `sconv_model`, or `fc_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  0\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 9.375 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.536341\n",
      "model loss is  0.645619\n",
      "model loss is  0.940639\n",
      "model loss is  0.378845\n",
      "model loss is  0.626495\n",
      "Iter:  1000\n",
      "32/32 [==============================] - 0s\n",
      "Your model is 15.625 percent accurate!\n",
      "\n",
      "\n",
      "model loss is  0.615656\n",
      "model loss is  0.58849\n",
      "model loss is  0.431542\n",
      "model loss is  0.520637\n",
      "model loss is  1.05763\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-d96f7d229834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimages_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m    956\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m    957\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 896\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    897\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1279\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1280\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1283\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/custom_tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1263\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "\n",
    "for i in range(num_steps):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iter: \", i)\n",
    "        evaluate(batch_size)\n",
    "        print('\\n')\n",
    "    \n",
    "    images_train, labels_train = next_batch(train_dataset, train_labels_encoded, batch_size)\n",
    "\n",
    "    loss = model.train_on_batch(images_train.reshape(batch_size, vsize, hsize, num_channels), labels_train) \n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"model loss is \", loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Now that we've seen all three models in action, let's make some actual predictions. Here, you can load test data and save predictions for the classic [Kaggle competition](https://www.kaggle.com/c/digit-recognizer/data). If you want to participate in the Kaggle challenge with any of these datasets, please go to the link and save the test.csv file to the cloned directory. You can also use other test datasets here instead, if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll evaluate the accuracy of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4992/5000 [============================>.] - ETA: 0sYour model is 95.86 percent accurate!\n"
     ]
    }
   ],
   "source": [
    "evaluate(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test dataset from CSV\n",
    "\n",
    "If you aren't using the Kaggle dataset, just skip this step and load your own in a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.genfromtxt('test.csv', delimiter=\",\", dtype=int)[1:,:].reshape(len(test), 28, 28, 1)\n",
    "\n",
    "test_dataset = test - np.mean(test, axis=0)\n",
    "\n",
    "print(\"Mean: %f, Std: %f\" % (np.mean(test_dataset), np.std(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions for Test Dataset\n",
    "\n",
    "This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions to file:\n",
    "\n",
    "This function will save your images to a csv file in the correct format for submission to the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_file(data, name):\n",
    "    index = np.linspace(1,len(data),len(data))\n",
    "    data = np.argmax(data, axis=1)\n",
    "    with open('{}.csv'.format(name), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(['ImageId', 'Label'])\n",
    "        for i in range(len(data)):\n",
    "            writer.writerow([index[i].astype(np.uint32), data[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_file(predictions, \"MNIST_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained version of the model\n",
    "\n",
    "Keras allows us to load and save entire models, so running this command will load a pretrained model which you can continue training or use for testing purposes. I have included a pre-trained model in the github repository, but you should generally only run this with your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('model_name.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to file\n",
    "\n",
    "If you'd like to save the current model for future use, run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "model.save('MNIST_model_{}'.format(datetime.datetime.now().strftime(\"%I:%M:%b:%d:%Y\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer Visualization\n",
    "\n",
    "Before you proceed, make sure you have run one of the second two models most recently, or load one of those models above. We will be viewing hidden layers in the MNIST networks, which will tell us something about how the network is parsing the information.\n",
    "\n",
    "Our first step is to create a new network, with just a single layer identical to the first layer in the convolution networks above. We initialize the layer with weights from the above network, and view that output next to the corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualization = Sequential()\n",
    "visualization.add(Conv2D(1, 3, strides=1, padding = 'same', input_shape=(vsize, hsize, num_channels), weights=model.layers[0].get_weights())) # for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUxsC3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ed85cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfqKgGH6ioBh+oqAYfqKgGH6ioP4PpE7P64QeuUwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e644f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-a1fa6fdaf7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "hidden_output = visualization.predict(train_dataset.reshape(len(train_dataset),vsize, hsize, num_channels))\n",
    "\n",
    "while True:\n",
    "    n = random.randint(0, len(train_dataset))\n",
    "    plt.imshow(train_dataset[n].reshape(vsize, hsize), cmap='Greys')\n",
    "    plt.show()\n",
    "    plt.imshow(hidden_output[n].reshape(vsize, hsize), cmap='Greys')\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we will view some of the deeper layers, although they will be less informative overall. **Note: this is designed to work with the second model. Please run that model, or change the depth of the second layer to 84**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualization2 = Sequential()\n",
    "\n",
    "visualization2.add(Conv2D(1, 3, strides=1, padding = 'same', input_shape=(vsize, hsize, num_channels), weights=model.layers[0].get_weights())) # for visualization\n",
    "model.add(LeakyReLU(0.2))\n",
    "\n",
    "visualization2.add(Conv2D(28, 1, strides=1, padding = 'same', weights=model.layers[2].get_weights()))\n",
    "\n",
    "hidden_output = visualization2.predict(train_dataset.reshape(len(train_dataset),vsize, hsize, num_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ea2a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOcXwEznF8BM5xfATOcXwEzn1f8nConcy+behAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ea13748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = random.randint(0, len(train_dataset)) # which image\n",
    "m = random.randint(0, 27) #which layer\n",
    "\n",
    "plt.imshow(hidden_output[n, m])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(train_dataset[n].reshape(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fascinating, although not that enlightening. Stil it's nice to see something of what's going on. For more information about visualization techniques, check out [this paper](https://arxiv.org/pdf/1311.2901.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this was useful, and helped you better understand how to implement convolution neural networks. Please let me know if you have any comments or ways of improving the notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:custom_tensorflow]",
   "language": "python",
   "name": "conda-env-custom_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
